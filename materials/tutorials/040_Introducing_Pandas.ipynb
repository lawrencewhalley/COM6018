{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 040 Processing Data with Pandas\n",
    "\n",
    "> COM6018\n",
    "\n",
    "*Copyright © 2023–2025 Jon Barker, University of Sheffield. All rights reserved.*\n",
    "\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "### 1.1 What is Pandas?\n",
    "\n",
    "Up to this point, we have seen that Python’s built-in lists and dictionaries give us a lot of flexibility in how we process data, but this approach is slow and quickly becomes impractical when working with large datasets. NumPy provides a much more efficient alternative for handling numerical data, offering powerful tools for mathematical operations, linear algebra, and the kinds of matrix and vector computations that are common in machine learning. However, NumPy is less effective when we need to deal with more complex, structured data that might include a mix of numbers, text, categories, or missing values. This is where **Pandas** becomes useful.\n",
    "\n",
    "**Pandas** is a third-party, open-source library that was created to meet the needs of data scientists. It builds on the speed of NumPy but adds a wide range of features for handling structured, table-like data, making it possible to work efficiently with datasets that resemble spreadsheets or database tables. While Pandas originally relied on NumPy as its foundation, NumPy was never designed with these kinds of tasks in mind, which led to some inefficiencies. To address this, the recent release of Pandas 2.0 introduced the option of replacing the NumPy backend with [Apache Arrow](https://arrow.apache.org/), a framework developed specifically for the fast and efficient processing of large, table-based datasets.\n",
    "\n",
    "This tutorial is based on Pandas 2.3. The complete documentation can be found at [https://pandas.pydata.org/docs/](https://pandas.pydata.org/docs/).\n",
    "\n",
    "### 1.2 Installing Pandas\n",
    "\n",
    "As Pandas is not part of the Python standard library, it must be installed before it can be used. This can be done easily using the `pip` package manager, or the `uv` environment manager that we are using with this module.\n",
    "\n",
    "If Pandas is not installed in your virtual environment, you can install it by typing the following command in your terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "uv add pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "This will install the latest version of Pandas.\n",
    "\n",
    "To use Pandas in your Python code, you simply need to import it at the start of your program, e.g.,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Conventional Python practice is to import Pandas using the alias `pd`. This is not essential, but it is recommended. Following existing conventions will make your code easier to read for other Python programmers.\n",
    "\n",
    "### 1.3 Basic Pandas Concepts\n",
    "\n",
    "Pandas is designed for processing structured data much like the data that would be stored in a spreadsheet or a CSV file. Pandas organises data using two basic data structures: the **Series** and the **DataFrame**. A **Series** is a one-dimensional array of data that can be thought of as a single column in a spreadsheet. A **DataFrame** represents a table of data, which is represented as a collection of **Series** objects. A **DataFrame** can be thought of as being similar to an entire spreadsheet. Behind the scenes, Pandas stores this data in NumPy arrays by default, although, as we will see later, it can also be configured to use Apache Arrow.\n",
    "\n",
    "Pandas has its own **type system** for representing data. This is largely the same as NumPy’s but extends it to handle additional types such as dates and intervals. Each Series (i.e., each column in a spreadsheet) has its own type, and different Series within the same DataFrame can use different types.\n",
    "\n",
    "Beyond data storage, Pandas provides powerful tools for processing data. It supports operations such as filtering, joining, and grouping, as well as convenient methods for reading and writing data in many formats, including CSV, Excel, JSON, and SQL. Pandas also includes some basic data visualisation features, which are generally easier to use than the equivalent functionality in `Matplotlib`.\n",
    "\n",
    "n the remainder of this tutorial, we will build on these basic concepts step by step. In Section 2, we will look at how to read data from common file formats such as CSV and JSON. Section 3 introduces the DataFrame object in more detail and shows how to select, filter, and inspect data. In Section 4, we will see how to compute summary statistics and group data using the split–apply–combine approach. Section 5 explains how to deal with missing data, while Section 6 demonstrates how to merge and join data from multiple sources.\n",
    "\n",
    "Sections 7 and 8 cover more advanced topics that are not core to this module but are included for completeness. Section 7 introduces the new Apache Arrow backend, which provides faster and more memory-efficient data handling for large datasets. Section 8 shows how to read data from SQL databases — while we will not be working with SQL in this module, it is useful to know that Pandas can easily read data from database tables.\n",
    "\n",
    "## 2. Reading Data\n",
    "\n",
    "Pandas has a range of methods for reading data from different standard data formats. For example, it can read Excel spreadsheets directly with a simple `read_excel()` method. However, continuing from our previous tutorials, we will focus on the commonly used CSV and JSON formats.\n",
    "\n",
    "### 2.1 Reading CSV Data\n",
    "\n",
    "Reading a CSV file with Pandas can be done with a single line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/040/windfarm.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "The `read_csv` method will parse the file and return the results as a Pandas DataFrame object. (It is common to use the variable name `df` for a DataFrame object in simple code examples. You may want to give the DataFrame a more descriptive name when writing more complex code.)\n",
    "\n",
    "We can now print the resulting DataFrame object to see what it contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "This will produce output that looks like the following,\n",
    "\n",
    "```text\n",
    "       id   \"turbines\"   \"height\"   \"power\"\n",
    "0  WF1355           13         53     19500.0\n",
    "1  WF1364            3         60      8250.0\n",
    "2  WF1356           12         60     24000.0\n",
    "3  WF1357           36         60     72000.0\n",
    "```\n",
    "\n",
    "Note that the `read_csv` method has read the names of the fields from the first line of the CSV file and used these to name the DataFrame columns. It has placed the names in quotes because quotes were present in the CSV file.\n",
    "\n",
    "If our CSV file did not have column names in the first line, then we would need to specify them using the `names` parameter, e.g.,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/040/windfarm.no_names.csv', names=['id', 'turbines', 'height', 'power'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "Note also how Pandas has added an extra column without a name to the left of the data. This is known as the **index column**. The index column is used to uniquely identify each row in the DataFrame. Pandas has generated this index for us by giving the rows consecutive integer index values starting from 0. If we want to use a different column as the index, then we can specify this using the `index_col` parameter, e.g.,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_id = pd.read_csv('data/040/windfarm.csv', index_col='id')\n",
    "print(df_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "This will produce output looking like the following,\n",
    "\n",
    "```text\n",
    "         \"turbines\"   \"height\"   \"power\"\n",
    "id\n",
    "WF1355           13         53   19500.0\n",
    "WF1364            3         60    8250.0\n",
    "WF1356           12         60   24000.0\n",
    "WF1357           36         60   72000.0\n",
    "```\n",
    "\n",
    "```{tip}\n",
    "You typically want the values in the index column to be unique. If they are not unique, then Pandas will still allow you to use the column as the index, but it will not be able to use some of the more advanced functionality that it offers for indexing. If you are not sure about which column to use as the index, then it is often best not to specify one and to just allow Pandas to create its own index column.\n",
    "```\n",
    "\n",
    "### 2.2 Reading JSON Data\n",
    "\n",
    "Reading JSON data is just as easy as reading CSV data. We can read the climate JSON data from our previous tutorial with a single line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json('data/040/climate.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "We can now print the resulting DataFrame object to see what it contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Note that the `monthlyAvg` data which was a list of dictionaries in the JSON file now appears in a single column of the DataFrame. This is not a very convenient format for processing the data. We can use a method called `apply()` to convert this column into a DataFrame with one column per dictionary in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_monthly = df['monthlyAvg'].apply(pd.Series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "This creates columns 0, 1, ... each containing a dict containing the statistics for that month.\n",
    "Using `apply()` a second time on one of the columns can expand the dictionaries into separate columns for each statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_monthly[0].apply(pd.Series))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "This is a little complicated, but it is typical of the preprocessing steps we might need to take if working with JSON files that have a complex nested structure. Fortunately, most of the data we will be working with will be in CSV format or in a simple JSON format with a flat structure.\n",
    "\n",
    "## 3. The DataFrame Object\n",
    "\n",
    "We will now look in a bit more detail at the DataFrame object. For this section we will be using a dataset that records the extent of sea ice in the Arctic and Antarctic. This data is available from the [National Snow and Ice Data Center](https://nsidc.org/data/g02135). The data are available in CSV format and we have already downloaded it and saved it in the `data` directory as `seaice.csv`.\n",
    "\n",
    "We will read the data and print the DataFrame,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/040/seaice.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "Note that when printing a DataFrame, Pandas will only show the first and last few rows and columns. We can see from the output that the DataFrame has 26,354 rows and measurements that date back to 1978. But we cannot see the full set of columns.\n",
    "\n",
    "To get a list of all the columns in the DataFrame, we can use the `columns` attribute,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "This will produce output that looks like the following,\n",
    "\n",
    "```text\n",
    "Index(['Year', ' Month', ' Day', '     Extent', '    Missing', ' Source Data',\n",
    "       'hemisphere'],\n",
    "      dtype='object')\n",
    "```\n",
    "\n",
    "We can now see that the data are composed of seven columns: year, month, day, extent, missing, source data, and hemisphere. The extent and missing columns record the extent of the sea ice and the number of missing measurements for each day. The source data column records the source of the data. The hemisphere column records whether the data are for the Arctic or Antarctic.\n",
    "\n",
    "Note that space characters have become incorporated into the column names. This is because the columns in the CSV file had spaces after the commas. We can easily skip these spaces by adding the `skipinitialspace=True` parameter to the `read_csv()` method,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/040/seaice.csv', sep=',', skipinitialspace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### 3.1 Selecting a Column\n",
    "\n",
    "We can select a single column from the DataFrame using the column name as an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "extent = df['Extent']\n",
    "print(extent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "This will produce output looking like the following,\n",
    "\n",
    "```text\n",
    "0        10.231\n",
    "1        10.420\n",
    "2        10.557\n",
    "3        10.670\n",
    "4        10.777\n",
    "          ...\n",
    "26349    10.085\n",
    "26350    10.078\n",
    "26351    10.219\n",
    "26352    10.363\n",
    "26353    10.436\n",
    "Name: Extent, Length: 26354, dtype: float64\n",
    "```\n",
    "\n",
    "Notice how this looks a bit different from when we printed the DataFrame. This is because the column is not a DataFrame; it is a Series. Remember, Pandas uses the Series object for storing individual columns of data. We can check this by printing the type of the column,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(df))  # This will print <class 'pandas.core.frame.DataFrame'>\n",
    "extent = df['Extent']\n",
    "print(type(extent))  # This will print <class 'pandas.core.series.Series'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "The Series object has a specific type (called its `dtype`). In this case the type is `float64` which means that the data are stored as 64-bit floating point numbers. We can check the type of the data in a Series using the `dtype` attribute. Or, more conveniently, we can retrieve the data type of all the columns in the DataFrame using the DataFrame's `dtypes` attribute. Let's do that now,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "This will produce output looking like the following,\n",
    "\n",
    "```text\n",
    "Year             int64\n",
    "Month            int64\n",
    "Day              int64\n",
    "Extent         float64\n",
    "Missing        float64\n",
    "Source Data     object\n",
    "hemisphere      object\n",
    "dtype: object\n",
    "```\n",
    "\n",
    "We can see that the Year, Month and Day have integer values; the Extent and Missing columns have floating-point values; and the Source Data and hemisphere columns have 'object' values.\n",
    "\n",
    "Pandas has inferred these types from the content of the CSV file. It usually guesses correctly, but it can sometimes get it wrong. If it is unsure it will typically default to the 'object' type. This has happened for the 'Source Data' and 'hemisphere' columns which should be strings. We can fix this by explicitly converting these columns to strings using the `astype()` method,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Source Data'] = df['Source Data'].astype(\"string\")\n",
    "df['hemisphere'] = df['hemisphere'].astype(\"string\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "A Series object is much like a list, and we can use indexes and slicing operators to reference elements in the Series. For example, we can get the first element in the Series using,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "extent = df['Extent']\n",
    "print(extent[0])  # print the first element"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "or we can retrieve the sequence of elements from the 6th to 10th with,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "extent = df['Extent']\n",
    "print(extent[5:10]) # print a sub-Series from index 5 to 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "(Recall that the first element of a Series has index 0, so the 10th element has index 9. The slice notation `a:b` means to go from index `a` to `b-1` inclusive.)\n",
    "\n",
    "### 3.2 Selecting a Row\n",
    "\n",
    "We can select a single *row* from the DataFrame using the `iloc` attribute. This attribute takes a single integer index and returns a Series containing the data from the row at that index. For example, to select the first row, we can use,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "Notice how the results have been returned as a Series object. We can check this by printing the type of the row,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(row)) # This will print <class 'pandas.core.series.Series'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "DataFrames are a collection of Series objects representing the columns, However, if we extract a single row, this will also be returned as a Series object, i.e., representing the values in that row. The dtype of the row series is 'object'. An 'object' type can basically store anything, so it is being used here because each element in the row Series has a different type.\n",
    "\n",
    "Alternatively, we can extract a range of rows using slicing,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = df.iloc[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "Note that a collection of rows is still a DataFrame. We can check this by printing the type of the variable `rows`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "Basically, if we have a single row or column, then we are dealing with a Series, but if we have multiple rows and columns, then we are dealing with a DataFrame.\n",
    "\n",
    "### 3.3 Filtering a DataFrame\n",
    "\n",
    "Very often in data science, we need to select some subset of our data based on some condition, e.g., all cities where the population is greater than 1 million or all years where the average temperature was less than 15 degrees Celsius. Or, more simply, we may wish to select every row that has an even index value. Alternatively, we may want to remove various columns from the DataFrame, i.e., those representing fields that we are not interested in. We will refer to these operations that take a DataFrame and return another smaller DataFrame as **filtering** operations. In this section, we will look at a few simple examples.\n",
    "\n",
    "#### 3.3.1 Removing or Selecting Columns\n",
    "\n",
    "After loading our data, we may want to remove columns that we are not interested in. This is easily done using the `drop()` method. For example, to make it easier to print the data frame, we will remove the Source Data column using,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['Source Data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "There are now only 6 columns in the DataFrame and they can all be viewed at once,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "Rather than removing columns, an alternative is to actively select the subset of columns that we wish to keep. This is done with the following indexing notation,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Year', 'Month', 'Day', 'Extent', 'Missing', 'hemisphere']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "#### 3.3.2 Selecting Rows\n",
    "\n",
    "Very often when analysing data we want to select a subset of the samples that match some criterion, i.e., to select a subset of the rows in our DataFrame.  This is very straightforward in Pandas and is done using a Boolean expression as the index value.\n",
    "\n",
    "For example, let's say we want to select all the rows where the sea ice extent value is less than 5.0. We can do this using,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = df['Extent'] < 5.0\n",
    "df_low_extent = df[selector]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "We have done this in two steps. The first line takes the Extent Series and compares every value to 5.0. This produces a Series of boolean (i.e., True or False) values which we have stored in a variable called `selector`. The second line uses this boolean Series as an index which has the effect of selecting only the rows for which the value is True.\n",
    "\n",
    "If we now print the resulting DataFrame we can see that it only contains about 3000 rows of the original 26,354 rows in the dataset; i.e., these are the samples where the sea ice extent has fallen below 5.0 billion square kilometres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_low_extent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "They are ordered by year. What do you notice about the earliest year of these data? Recall that the original complete dataset had recordings going back to 1978, but all the monthly recordings where the extent of the sea ice fallen below 5.0 billion square kilometres have occurred since 2007. Is this something that might have happened by chance, or is it evidence of a trend in the data? We will see how to answer such questions a little later in the module.\n",
    "\n",
    "Let us say that we now just want to look at the Arctic data. We can do this by selecting only the rows where the hemisphere is 'north'. We can do this using the `eq()` method,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter = df['hemisphere'].eq('north')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "If we wanted to combine these two filters to select only the rows where the sea ice extent is less than 5.0 billion square kilometres and the hemisphere is 'north', we could do this using the `&` operator,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter = (df['hemisphere'].eq('north')) & (df['Extent'] < 5.0)\n",
    "df_low_extent_north = df[filter]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    ":::{caution}\n",
    "When combining multiple filter expression use the operators `&` (and), `|` (or) and `~` (not) rather than the keywords `and`, `or` and `not`. The latter keywords will not work for combining filter expressions.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_low_extent_north)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {},
   "source": [
    "This leaves just 339 rows.\n",
    "\n",
    "```{note}\n",
    "Be careful to use parentheses around each filter expression above. The `&` operator has a higher precedence than the `<` operator so if you do not use parentheses then it will still be a valid expression but it will not work in the way you expected.\n",
    "```\n",
    "\n",
    "Note that we use a very similar indexing syntax, i.e., `df[selector]`, for selecting both rows and columns. The difference is that when selecting rows we use a boolean series as the index value, whereas when selecting columns we use a list of column names as the index value. This can be a little confusing at first, but you will soon get used to it.\n",
    "\n",
    "## 4. Grouping and averaging values\n",
    "\n",
    "Up to this point, we have mainly focused on how to read, inspect, and filter data in Pandas. In practice, however, much of data analysis involves summarising information — for example, finding averages, totals, or other statistics that describe entire columns or subsets of the data. Pandas provides concise and efficient ways to compute such statistics, either across all rows or within meaningful groups (such as by year, category, or location).\n",
    "\n",
    "In this section, we will first look at simple operations that compute statistics over entire columns, and then explore how to group data so that these statistics can be computed separately for each group.\n",
    "\n",
    "### 4.1 Operating on columns\n",
    "\n",
    "There are many operations that we might apply over an entire column to compute some 'statistic' of that column. For example, we might want to compute a columns minimum, maximum, mean, median, standard deviation, etc. The Series object provides many methods to make this easy.\n",
    "\n",
    "For example, to compute the mean of the Extent column we can use,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_value = df['Extent'].mean()\n",
    "print(mean_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "We can also calculate the minimum and maximum values in a similar way,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['Extent'].min(), df['Extent'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "For convenience, the DataFrame object provides a `describe()` method that will compute and display the basic statistics for all columns in the DataFrame that have numerical values. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "This will print the following table:\n",
    "\n",
    "```text\n",
    "               Year         Month           Day        Extent       Missing\n",
    "count  26354.000000  26354.000000  26354.000000  26354.000000  26354.000000\n",
    "mean    2000.591941      6.507399     15.740685     11.494986      0.000003\n",
    "std       10.896821      3.451938      8.801607      4.611734      0.000227\n",
    "min     1978.000000      1.000000      1.000000      2.080000      0.000000\n",
    "25%     1992.000000      4.000000      8.000000      7.601000      0.000000\n",
    "50%     2001.000000      7.000000     16.000000     12.217000      0.000000\n",
    "75%     2010.000000     10.000000     23.000000     15.114000      0.000000\n",
    "max     2019.000000     12.000000     31.000000     20.201000      0.024000\n",
    "```\n",
    "\n",
    "The table shows the number of items (count), mean, standard deviation (std), minimum (min), maximum (max) and the 25th, 50th, and 75th percentiles of the data in each column. Note that the 50th percentile is the same as the median. These statistics will be more meaningful for some columns than for others. For example, in the above the mean and standard deviation of the Month and Day columns are not very meaningful. Nevertheless, the `describe()` method is very useful for getting a quick overview of the data and is often the first thing we will do when we load a new dataset.\n",
    "\n",
    "### 4.2 Grouping data\n",
    "\n",
    "n the previous section, we computed statistics across entire columns of the dataset. However, it is often more useful to calculate these statistics separately for meaningful subsets of the data — for example, to find the mean sea-ice extent for each year, each month, or for the northern and southern hemispheres independently. To do this, we need a way to divide the data into groups and then compute statistics within each group.\n",
    "\n",
    "Pandas provides a simple and consistent pattern for this type of operation, often referred to as the \"split–apply–combine\" process:\n",
    "\n",
    "  1. Split (group): divide the DataFrame into groups based on one or more key columns.\n",
    "  2. Apply (process): perform an operation independently on each group — for example, computing a mean, minimum, or count.\n",
    "  3. Combine: bring the results of these operations back together into a new DataFrame or Series.\n",
    "\n",
    "This split–apply–combine pattern is exactly the sequence that Pandas follows when we use the groupby() method. We first tell Pandas how to split the data into groups, then apply one or more operations to each group, and finally combine the results into a new object for inspection or further analysis.\n",
    "\n",
    "In the combine step, Pandas also creates an index that reflects the grouping keys. When you group by more than one column, the resulting object will use a hierarchical or “MultiIndex,” where each level corresponds to one of the grouping columns.\n",
    "\n",
    "For example, to group the data by month we can use,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby('Month')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "This returns a special `DataFrameGroupBy` object.  This object does not immediately compute anything — it simply records how the data have been split into groups. We can then compute statistics on those groups by selecting a column and using the same methods that we used earlier. For example, to compute the mean sea-ice extent for each month we can use,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grouped['Extent'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {},
   "source": [
    "TThe code above produces a mean value for each month of the year (1 is January, 2 is February, etc.). Notice how the amount of ice reaches a maximum in November and a minimum in February. This might seem somewhat surprising — the maximum and minimum are not six months apart, as might be expected. Note, however, that when we grouped by month we included both Northern and Southern Hemisphere measurements, so this mean will be the average across both hemispheres, which can lead to an unintuitive result.\n",
    "\n",
    "Let us say that we wanted to look at the sea-ice extent for each month but separately for the northern and southern hemispheres. We now want to group by both month and hemisphere. We can do this easily by passing a list of column names to the `groupby()` method,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby(['hemisphere', 'Month'])\n",
    "print(grouped['Extent'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78",
   "metadata": {},
   "source": [
    "When grouping by multiple fields, the combine step produces a result indexed by both grouping keys. In Pandas, this is represented as a 'MultiIndex', where the first level corresponds to the first grouping column (hemisphere) and the second level corresponds to the second grouping column (Month). You can later reset this MultiIndex using `reset_index()` if you prefer to work with a flat DataFrame.\n",
    "\n",
    "For example, if we want to convert the grouped means into an ordinary DataFrame with regular columns instead of a hierarchical index, we can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby(['hemisphere', 'Month'])\n",
    "monthly_means = grouped['Extent'].mean().reset_index()\n",
    "print(monthly_means.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80",
   "metadata": {},
   "source": [
    "This produces a DataFrame where hemisphere, Month, and Extent are all standard columns rather than index levels. This makes the data easier to manipulate or save to a file.\n",
    "\n",
    "We now see that in the Arctic, sea ice extent reaches a maximum monthly average in March and a minimum in September. In the Antarctic, sea ice extent reaches a maximum month average in September and a minimum in February. This is more in line with what we might expect — that is, the maxima and minima are about six months apart. The dates might seem a little later than what you would consider to be the peak of winter and summer. This is because of the way water stores heat: the oceans take a long time to cool down and warm up, so the coldest and warmest months are not necessarily those with the shortest and longest days. You can now also see that the Antarctic sea-ice extent is much more variable than the Arctic sea-ice extent.\n",
    "\n",
    "Let us now consider how we would examine the Arctic and Antarctic sea-ice minimum over the years. We can do this by grouping by hemisphere and year and then selecting the minimum value in each group,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby(['hemisphere', 'Year'])\n",
    "print(grouped['Extent'].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82",
   "metadata": {},
   "source": [
    "(Here again, the result uses a MultiIndex: hemisphere and year.)\n",
    "\n",
    "Pandas has built-in methods to make plots. Below we are making a plot and then getting the `figure` from the plot and saving it to a file. The resulting plot is being imported back into these notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby(['hemisphere', 'Year'])\n",
    "ice_extent = grouped['Extent'].min()['north']\n",
    "\n",
    "my_plot = ice_extent.plot()\n",
    "my_plot.get_figure().savefig(\"figures/040_1.jpeg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84",
   "metadata": {},
   "source": [
    "It will generate the following plot.\n",
    "\n",
    "![Arctic sea ice minimum](figures/040_1.jpeg)\n",
    "\n",
    "You can see that the sea ice extent has been broadly trending downwards but the values at the extremes of the x-axis look a bit odd, i.e., there are very large values for the first and last year in the dataset. This is an 'artifact' caused by the data for these years being incomplete. The data for 1978 only start in November and the data for 2019 only go up to July. The code has computed the minimum for the few months that are available, but this is not representative of the minimum for the full year. We can fix this problem by simply removing these years from the data before plotting.\n",
    "\n",
    "To remove the first and last values in a Series we can use the familiar slicing notation that we saw when using NumPy, '[1:-1]'. So, the code will now look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby(['hemisphere', 'Year'])\n",
    "ice_extent = grouped['Extent'].min()['north']\n",
    "ice_extent = ice_extent[1:-1]  # remove first and last years\n",
    "\n",
    "my_plot = ice_extent.plot(title='Arctic sea ice minimum', ylabel='million square kilometres (million km²)', xlabel='Year')\n",
    "my_plot.get_figure().savefig(\"figures/040_2.jpeg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86",
   "metadata": {},
   "source": [
    "It will generate the following plot.\n",
    "\n",
    "![Arctic sea ice minimum](figures/040_2.jpeg)\n",
    "\n",
    "Notice how the sea ice minimum fluctuates seemingly at random from year to year. This kind of unexplained variation in data is often referred to as noise. However, ignoring the fluctuations, we can see that there also appears to be a downward trend in the sea ice minimum. If we want to be sure that there is a genuine downward trend and that the low values in recent years are not just part of the usual random fluctuations, then we need to perform some statistical analysis. We will look at how to do this later in the module.\n",
    "\n",
    "## 5. Dealing with Missing Data\n",
    "\n",
    "Very often we need to work with datasets that have 'missing values', i.e., for one or more fields in the data there is no value available. These are typically encoded in the data file using a special symbol such as 'NaN'. For example, we may have some employee data that records an ID, a surname and age, and a gender, but there may be some employees that have not provided either their age or gender or both.\n",
    "\n",
    "We can read and display some example data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/040/employee.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88",
   "metadata": {},
   "source": [
    "Note, below, the presence of 'NaN' in the age and gender columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90",
   "metadata": {},
   "source": [
    "There are some methods to test for the presence of these missing values. For example, the `isnull()` method will produce a new DataFrame of boolean values with a value of True in every location where an item of data is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.isnull(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92",
   "metadata": {},
   "source": [
    "Typically if we have missing values we will need to take some action to deal with them before progressing with our analysis. Depending on the situation, the most appropriate action might be to simply remove the rows with missing values, or to fill in the missing values with some sensible default value, or to use some more sophisticated method for estimating what the missing values would have been if they had been present.\n",
    "\n",
    "### 5.1 Filling missing data with a fixed value\n",
    "\n",
    "Sometimes our data will contain missing values, which Pandas represents as NaN. We can replace these with something more useful or readable using the `fillna` method. For instance, if we want to display the data and make it clearer, we might choose to replace missing values in the Gender column with the string \"Gender not provided\".\n",
    "\n",
    "This can be done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna({\"Gender\":\"Gender not provided\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94",
   "metadata": {},
   "source": [
    "Here, we pass a dictionary to `fillna`, where the key is the column name (\"Gender\") and the value is what we want to use as the replacement. The `inplace=True` argument means the DataFrame will be updated directly, without creating a new copy.\n",
    "\n",
    "After the replacement, our DataFrame will be as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96",
   "metadata": {},
   "source": [
    "The idea of modifying objects in place is common in Pandas, and many methods provide an `inplace` parameter. For large DataFrames, modifying data in place can reduce memory overhead because it avoids creating a new copy. However, this style of programming can also introduce subtle bugs. For example, if a function both modifies a DataFrame in place and returns it, the caller might not realise that the original DataFrame they passed in has already been altered. This kind of unexpected behaviour can be difficult to track down.\n",
    "\n",
    "A safer and more transparent approach is to leave `inplace=False` (the default) and explicitly assign the result of the operation to a new variable. This “copy semantics” style makes it clearer what is happening and is generally recommended unless you are certain that performance or memory usage requires the in-place approach. In fact, the Pandas developers are moving away from the use of `inplace` altogether, and some methods in recent versions no longer support it.\n",
    "\n",
    "For example, the same operation can be written using copy semantics like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df.fillna({\"Gender\":\"Gender not provided\"})\n",
    "print(new_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98",
   "metadata": {},
   "source": [
    "The `fillna` method has other useful options such as a `method` parameter that can fill the missing value with the previous or next valid observation in the Series. This behaviour is useful for sequential data, e.g., data ordered by date, for example. For example, if there is a weather station but it has failed to record a temperature on a particular day, then we might opt to fill in the missing value with the temperature recorded on the previous day as this is likely to be a reasonably good estimate.\n",
    "\n",
    "### 5.2 Filling missing values using 'mean imputation'\n",
    "\n",
    "Another common strategy for filling missing values in numeric data is to fill the missing values with the mean of the values that are present. In the data science literature, this is known as 'mean imputation'. This might be a sensible way of treating our missing employee age data and can be easily done using,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna({'Age':df['Age'].mean()}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100",
   "metadata": {},
   "source": [
    "This will produce the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102",
   "metadata": {},
   "source": [
    "Note how the missing age values for Gonzalez and Nguyen have now been replaced with `36.5`.  Obviously, this is unlikely to be their true ages, but it might be a sensible guess to use in the analysis stages that are going to follow.\n",
    "\n",
    "Treatment of missing data is a big topic and something we will return to later in the module.\n",
    "\n",
    "## 6. Collating data\n",
    "\n",
    "Very often, as Data Scientists, we need to combine data from different sources. For example, we might have two separate datasets that are linked by a shared field, such as the customer ID, date, or location. These will typically start life as separate files. These can be read into two separate DataFrames but in order to understand the data we will generally need to combine these DataFrames into a new common DataFrame, in some way.\n",
    "\n",
    "In the simple example below, we will consider two different sets of atmospheric gas readings, carbon dioxide and methane that have been measured at the same location over time but are stored separately in the files `co2.csv` and `ch4.csv`. (The file names are based on the chemical formulae for carbon dioxide and methane, CO<sub>2</sub> and CH<sub>4</sub>, respectively).\n",
    "\n",
    "Let us imagine that the first few rows of these tables look like this\n",
    "\n",
    "```csv\n",
    "year, month, day, co2\n",
    "2020, 1, 02, 442\n",
    "2020, 1, 03, 444\n",
    "2020, 1, 04, 441\n",
    "2020, 1, 07, 446\n",
    "```\n",
    "\n",
    "and this,\n",
    "\n",
    "```csv\n",
    "year, month, day, ch4\n",
    "2020, 1, 02, 442\n",
    "2020, 1, 03, 444\n",
    "2020, 1, 05, 441\n",
    "2020, 1, 06, 442\n",
    "2020, 1, 07, 446\n",
    "```\n",
    "\n",
    "We can see that both tables share the same year, month and day columns but each has its own unique column for the gas concentration measurement. We can also see that the data are not recorded on every day: on some days there is both a carbon dioxide and a methane measurement, on some there is only one or the other and on some dates there is neither. We will now consider different ways in which these datasets can be combined.\n",
    "\n",
    "### 6.1 Combining DataFrames using the `merge` method\n",
    "\n",
    "To make the task a bit more instructive, the above data have some missing entries, e.g., entries for days 05 and 06 are missing from the CO<sub>2</sub> data. The entry for day 04 is missing from the CH<sub>4</sub> data. In a well-designed dataset these might have been recorded with NaN values but often there are just gaps in the data.\n",
    "\n",
    "We can read these tables using,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {},
   "outputs": [],
   "source": [
    "co2_df = pd.read_csv('data/040/co2_ex.csv', skipinitialspace=True)\n",
    "ch4_df = pd.read_csv('data/040/ch4_ex.csv',  skipinitialspace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(co2_df)\n",
    "print(ch4_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105",
   "metadata": {},
   "source": [
    "We can now `merge` the tables. To do this, we need to pick a column to **merge on**. This is typically a key-like value that uniquely identifies the data entry (i.e., the row) and appears in both tables. If no one column is unique then we can use a collection of columns to merge on. For example, in our data, neither the year, month nor day would be a unique identifier if taken in isolation, but the three together form a unique set of values (i.e., because there is only one measurement per day, no two rows have an identical year, month and day). So we will use `on=['year', 'month', 'day']`.\n",
    "\n",
    "We can then merge in four different ways: `inner`, `outer`, `left` or `right`. These differ in how they treat missing values: `inner` will only merge rows which exist in both DataFrames (i.e., if you think of this a merging two *sets* then this would be an 'intersection'); `outer` will include rows that exist in either of the two DataFrames (i.e., in terms of sets, this would be a 'union'); `left` only includes rows which are in the first DataFrame and `right` only those in the second. This can be most easily understood by comparing the outputs below.\n",
    "\n",
    "* Merging CO<sub>2</sub> and CH<sub>4</sub> using an `inner` merge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = co2_df.merge(ch4_df, on=['year', 'month', 'day'], how='inner')\n",
    "print(combined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107",
   "metadata": {},
   "source": [
    "* Merging CO<sub>2</sub> and CH<sub>4</sub> using an `outer` merge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = co2_df.merge(ch4_df, on=['year', 'month', 'day'], how='outer')\n",
    "print(combined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109",
   "metadata": {},
   "source": [
    "* Merging CO<sub>2</sub> and CH<sub>4</sub> using a `left` merge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = co2_df.merge(ch4_df, on=['year', 'month', 'day'], how='left')\n",
    "print(combined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111",
   "metadata": {},
   "source": [
    "* Merging CO<sub>2</sub> and CH<sub>4</sub> using a `right` merge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = co2_df.merge(ch4_df, on=['year', 'month', 'day'], how='right')\n",
    "print(combined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113",
   "metadata": {},
   "source": [
    "In all cases, in the resulting table, any missing values will be automatically filled in with NaNs.\n",
    "\n",
    "**Warning**: Typically, the join columns act as a unique key. What happens if we choose columns that are not unique? Below we attempt to join on the 'year' column which is actually the same for all our rows, i.e.,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = co2_df.merge(ch4_df, on=['year'], how='inner')\n",
    "print(combined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115",
   "metadata": {},
   "source": [
    "This has taken the set of rows from each table that share the same year and made new rows using every possible pairing of rows in these sets. Also because 'month' and 'day' were not in the join, the new DataFrame now has separate month and day columns coming from the two original DataFrames which have been automatically distinguished with the suffixes '_x' and '_y'.  There are cases when we might want this behaviour, but if your output is looking like this, it is more likely that it's because you have set the `on` parameter incorrectly.\n",
    "\n",
    "### 6.2 Combining DataFrames using the `join` method\n",
    "\n",
    "The `join` method is very similar to `merge` except that it uses the DataFrame index to decide which rows to combine. It can be thought of as a special case of `merge`. However, it is often more convenient and faster to use. If the DataFrame already has a meaningful index, then this is very easy to use; e.g., perhaps a product ID column is being used as the index and we have two DataFrames containing product sales for different product ranges that we wish to join.\n",
    "\n",
    "In our case, if working with the CO<sub>2</sub> and CH<sub>4</sub> data from above, in order to use join, we would first need to make an index using the year, month, and day columns. We can do this using the `set_index` method, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116",
   "metadata": {},
   "outputs": [],
   "source": [
    "co2_df_indexed = co2_df.set_index(['year', 'month', 'day'])\n",
    "ch4_df_indexed = ch4_df.set_index(['year', 'month', 'day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(co2_df_indexed)\n",
    "print(ch4_df_indexed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118",
   "metadata": {},
   "source": [
    "We can now perform the join. Again, as with merge, we can join using `inner`, `outer`, `left` or `right`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = co2_df_indexed.join(ch4_df_indexed, how='outer')\n",
    "print(combined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120",
   "metadata": {},
   "source": [
    "Comparing `merge` and `join`, `merge` is more versatile because it can merge on any column, whereas join can only merge on the index. So you might wonder why we have a `join` method at all? The answer is that the `join` method works internally in a very different way from `merge` that exploits special properties of the DataFrame index. For large DataFrames, `join` is generally much faster than `merge` and so should be preferred if its use is possible.\n",
    "\n",
    "## 7. Using the Apache Arrow backend\n",
    "\n",
    "Pandas 2.x can store DataFrame columns using Apache Arrow instead of NumPy. Arrow provides efficient, columnar, nullable types (including true nullable integers) and can speed up I/O and many column operations. Some key benefits are:\n",
    "\n",
    "* **Faster operations** – many columnar operations (e.g., filtering, arithmetic) can be executed more efficiently on Arrow data.\n",
    "* **Smaller memory footprint** – Arrow uses compact, type-specific encodings and lightweight null bitmaps, reducing memory use for large DataFrames.\n",
    "* **True nullable types** – integers, booleans, and other types can natively represent missing values without falling back to slower object arrays.\n",
    "* **Better I/O performance** – Arrow-backed DataFrames read and write compact binary file formats like `Parquet` and `Feather` more quickly, often avoiding costly conversions.\n",
    "* **Cross-language interoperability** – Arrow is a standard across Python, R, Spark, and other systems, enabling zero-copy data sharing without duplication.\n",
    "\n",
    "If you are using `uv` to manage your environment you can run,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121",
   "metadata": {},
   "outputs": [],
   "source": [
    "uv add pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122",
   "metadata": {},
   "source": [
    "### 7.1 Reading data with Arrow-backed dtypes\n",
    "\n",
    "Let’s load the sea ice dataset again (as in Section 3), but now selecting the `pyarrow` backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/040/seaice.csv', sep=',', skipinitialspace=True,  dtype_backend=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125",
   "metadata": {},
   "source": [
    "Check dtypes—notice the Arrow annotations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127",
   "metadata": {},
   "source": [
    "You’ll typically see things like:\n",
    "\n",
    "* int64[pyarrow] for integers (nullable by design),\n",
    "* float64[pyarrow] for floats,\n",
    "* string[pyarrow] for text columns (instead of object).\n",
    "\n",
    "This means you no longer need special nullable integer dtypes (like Int64)—Arrow’s integer dtypes are nullable out of the box.\n",
    "\n",
    "### 7.2 Working with missing data (same APIs, Arrow dtypes)\n",
    "\n",
    "All the missing-data tools you used earlier still work. For example, with the employee dataset from Section 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_emp = pd.read_csv('data/040/employee.csv',  dtype_backend=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_emp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130",
   "metadata": {},
   "source": [
    "Fill missing values in Gender (same as before):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emp2 = df_emp.fillna({\"Gender\": \"Gender not provided\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_emp2)\n",
    "print(df_emp2.dtypes)  # Note string[pyarrow] instead of object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133",
   "metadata": {},
   "source": [
    "Mean imputation for a numeric column also works as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emp3 = df_emp.fillna({\"Age\": df_emp[\"Age\"].mean()})\n",
    "print(df_emp3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135",
   "metadata": {},
   "source": [
    "### 7.3 Grouping and aggregations (same code, Arrow underneath)\n",
    "\n",
    "Using the sea ice data again, the groupby logic from Section 4 is unchanged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/040/seaice.csv', sep=',', skipinitialspace=True,  dtype_backend=\"pyarrow\")\n",
    "grouped = df.groupby(['hemisphere', 'Year'])\n",
    "print(grouped['Extent'].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137",
   "metadata": {},
   "source": [
    "You can still plot, slice, and describe as before. The main difference is improved null handling and potentially better performance with Arrow-backed columns.\n",
    "\n",
    "### 7.4 Merge and join with Arrow-backed frames\n",
    "\n",
    "Revisit the CO₂/CH₄ example from Section 6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "co2_df = pd.read_csv('data/040/co2_ex.csv', skipinitialspace=True,  dtype_backend=\"pyarrow\")\n",
    "ch4_df = pd.read_csv('data/040/ch4_ex.csv', skipinitialspace=True,  dtype_backend=\"pyarrow\")\n",
    "\n",
    "combined_inner = co2_df.merge(ch4_df, on=['year','month','day'], how='inner')\n",
    "combined_outer = co2_df.merge(ch4_df, on=['year','month','day'], how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_inner)\n",
    "print(combined_outer)\n",
    "print(combined_outer.dtypes)  # Arrow dtypes will be shown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140",
   "metadata": {},
   "source": [
    "And with index-based join:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141",
   "metadata": {},
   "outputs": [],
   "source": [
    "co2_idx = co2_df.set_index(['year','month','day'])\n",
    "ch4_idx = ch4_df.set_index(['year','month','day'])\n",
    "joined = co2_idx.join(ch4_idx, how='outer')\n",
    "print(joined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142",
   "metadata": {},
   "source": [
    "The semantics are identical to NumPy-backed DataFrames; Arrow simply changes the underlying storage and dtypes.\n",
    "\n",
    "### 7.5 Arrow I/O: Parquet and Arrow Tables\n",
    "\n",
    "One major advantage of Arrow is its tight integration with modern binary, columnar file formats such as `Parquet`. Because Arrow and Parquet share the same underlying columnar representation, large datasets can be written and read directly without costly type conversions. This makes serialization faster, reduces storage size, and ensures efficient interoperability with other data-processing systems like Spark, Dask, and cloud data warehouses.\n",
    "\n",
    "Below we read our sea ice data from a human-readable CSV file and then save it to a binary `parquet` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/040/seaice.csv', sep=',', skipinitialspace=True, dtype_backend=\"pyarrow\")\n",
    "df.to_parquet(\"data/040/seaice.parquet\")  # fast, columnar, compressed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144",
   "metadata": {},
   "source": [
    "The `seaice.parquet` file is 415 kB compared to 4.1 MB for the equivalent CSV file - i.e., about 1/10th of the size.\n",
    "\n",
    "You can read it back in just as easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_back = pd.read_parquet(\"data/040/seaice.parquet\")\n",
    "print(df_back.dtypes)  # remains Arrow-backed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "`read_parquet` will preserve Arrow dtypes automatically when using Pandas 2.x.\n",
    ":::\n",
    "\n",
    "If you need a native Arrow table (e.g., to interoperate with other Arrow tools), you can convert via PyArrow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "table = pa.Table.from_pandas(df, preserve_index=False)\n",
    "print(table.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148",
   "metadata": {},
   "source": [
    "## 8. Reading from SQL databases\n",
    "\n",
    "So far we’ve loaded data from CSV files, JSON, and web APIs. Pandas can also connect directly to **relational databases**—the kind of systems you’ll often encounter in real organisations (e.g., SQLite, MySQL, PostgreSQL, or SQL Server).\n",
    "\n",
    "The general idea is the same:\n",
    "\n",
    "  1. Open a connection to the database.\n",
    "  2. Run an SQL query that selects the data you want.\n",
    "  3. Pull the results straight into a pandas DataFrame for analysis.\n",
    "\n",
    "### Example 1 - Reading from a SQLite database\n",
    "\n",
    "SQLite stores a whole database in a single file on disk—there’s no server, login, or setup needed.\n",
    "It’s ideal for portable datasets and demonstrations.\n",
    "\n",
    "In the example below we are using the `Chinook_Sqlite.sqlite` sample from the Chinook sample database.This is a publicly available example dataset originally created by L. Rocha for demonstrating relational database concepts. It contains fictional data for a digital music store, including information about artists, albums, tracks, customers, and invoices.\n",
    "\n",
    ":::{note}\n",
    "**Source:** <https://github.com/lerocha/chinook-database>\n",
    "**License:** Public domain / freely redistributable for educational use.\n",
    "**Copyright:** © 2008–2016 L. Rocha.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3, pandas as pd\n",
    "\n",
    "conn = sqlite3.connect(\"data/040/Chinook_Sqlite.sqlite\")\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "    Track.TrackId,\n",
    "    Track.Name AS TrackName,\n",
    "    Artist.Name AS Artist,\n",
    "    Album.Title AS AlbumTitle,\n",
    "    Genre.Name AS Genre,\n",
    "    Track.Milliseconds / 60000.0 AS DurationMinutes,\n",
    "    Track.UnitPrice,\n",
    "    Customer.LastName AS CustomerLastName,\n",
    "    Customer.Country\n",
    "FROM Track\n",
    "JOIN Album      ON Track.AlbumId = Album.AlbumId\n",
    "JOIN Artist     ON Album.ArtistId = Artist.ArtistId\n",
    "JOIN Genre      ON Track.GenreId = Genre.GenreId\n",
    "JOIN InvoiceLine ON Track.TrackId = InvoiceLine.TrackId\n",
    "JOIN Invoice    ON InvoiceLine.InvoiceId = Invoice.InvoiceId\n",
    "JOIN Customer   ON Invoice.CustomerId = Customer.CustomerId\n",
    "LIMIT 2000;\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150",
   "metadata": {},
   "source": [
    "This pulls a joined view of tracks, albums, artists, genres, and customers from the Chinook sample database and turns it into a DataFrame.\n",
    "From here you can use any pandas tools—groupby, plotting, filtering, and so on—exactly as if the data came from a CSV.\n",
    "\n",
    "### Example 2 - Reading from a remove MySQL database\n",
    "\n",
    "In practice, company data is usually held in a database server on a network. Pandas can connect to these systems too, via **SQLAlchemy** and the appropriate database driver.\n",
    "\n",
    "You will need to install the required packages first,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151",
   "metadata": {},
   "outputs": [],
   "source": [
    "uv add sqlalchemy pymysql"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152",
   "metadata": {},
   "source": [
    "The code below connects to the Rfam public MySQL database (read-only, no password required) and runs a short query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "engine = create_engine(\"mysql+pymysql://rfamro@mysql-rfam-public.ebi.ac.uk:4497/Rfam\")\n",
    "df = pd.read_sql(\"SELECT rfam_acc, rfam_id, description FROM family LIMIT 10\", engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155",
   "metadata": {},
   "source": [
    "Here, create_engine builds a connection string that includes:\n",
    "\n",
    "* the database type (mysql),\n",
    "* the Python driver (pymysql),\n",
    "* the credentials (rfamro), and\n",
    "* the server address and port.\n",
    "\n",
    "The rest is the same pattern—pd.read_sql() sends the SQL to the server, retrieves the results, and builds a pandas DataFrame.\n",
    "\n",
    "## Summary\n",
    "\n",
    "Pandas is a large and sophisticated package for working with data. In this tutorial, we have used it to read data, to deal with missing values, to conditionally filter data, to compute statistics on fields with and without prior grouping, and to combine datasets using merge and join. This is a good overview of the most commonly used functionality, but only scratches the surface of what Pandas can do. There are entire books dedicated to Pandas and extensive documentation available online: it is a very useful tool to have in your toolbox. We will be using it throughout the rest of the module and will introduce new functionality as required.\n",
    "\n",
    "*Copyright © 2023–2025 Jon Barker, University of Sheffield. All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "incorrectly_encoded_metadata,-all",
   "cell_metadata_json": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
