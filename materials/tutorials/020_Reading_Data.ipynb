{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 020 Reading and Writing Data Files\n",
    "\n",
    "> COM6018\n",
    "\n",
    "*Copyright Â© 2023â€“2025 Jon Barker, University of Sheffield. All rights reserved.*\n",
    "\n",
    "\n",
    "## 1 Introduction\n",
    "\n",
    "### 1.1 Storing data in files\n",
    "\n",
    "In this course, we will be dealing with many different sets of data. Almost all of these will start life as a file stored on a computer, which we will need to read into our programs. The programs that we write will be transforming or analysing these data in some way. The results will then typically need to be written back to a file so that they can be used by other programs or people. So, understanding how to read and write data files is a key skill for a data scientist.\n",
    "\n",
    "When storing data in a file, we need to use a standard format so that the data can be easily read by others. There are a huge variety of different data file formats that are used in data science; some are very general purpose, others are specialised `domain-specific' formats. In these notes, we will be introducing some of the most common formats that you are likely to encounter.\n",
    "\n",
    "### 1.2 Human-readable vs. Machine-readable\n",
    "\n",
    "One of the first considerations when writing data to a file is whether or not it needs to be **human-readable**. By human-readable, we mean that the data can be directly inspected and understood by a human, i.e., typically by being represented using ASCII characters. In many cases, this is not required and we can use a **machine-readable** format (also known as a **binary format**). A binary format will typically be more compact (i.e., it will consume less storage space) and faster to read and write. However, for small datasets the storage and read/write times are not significant, and a human-readable format is often easier to work with. In this course, we will start with a relatively small dataset and use human-readable formats for all of our data files.\n",
    "\n",
    "There are many different human-readable data formats, but two of the most common ones for data science are CSV and JSON. We will be looking at these formats in some detail in these notes.\n",
    "\n",
    "### 1.3 Some terminology\n",
    "\n",
    "Before describing the different data formats, we need to introduce some terminology. When we talk about a dataset, we typically refer to a collection of **data items** describing the objects in the set. Each data item is described by a collection of **fields**, where each field describes a different aspect of the object. For example, a dataset might contain information about a number of people. Each person would be a data item and the fields would be things like name, age, height, weight, etc. Note that the fields may be of different types; e.g., the name field would be a string, the age field would be an integer, the height field would be a float, etc.\n",
    "\n",
    "In data science, and particularly in machine learning, we are often dealing with datasets that represent examples of some larger population. For example, we may have a collection of 10,000 images of cats from which we want to learn about all possible images of cats. In machine learning, each data item is more commonly called a **sample** and the fields are called **features**. So depending on the context (or who we are talking to!) we might talk about a dataset containing 10,000 samples each with 100 features; or a dataset having 10,000 data items each with 100 fields.\n",
    "\n",
    "In these notes on file formats, we will use the terms 'data item' and 'field.' Later in the course, when discussing machine learning, we will more often use the terms 'sample' and 'feature.'\n",
    "\n",
    "## 2 CSV Files\n",
    "\n",
    "### 2.1 The CSV file format\n",
    "\n",
    "CSV stands for 'Comma Separated Values'. It is a very simple format that is commonly used for tabulated data, i.e. data that might be stored in a spreadsheet. It is therefore ideal for our typical case, where we have a dataset containing a number of data items, each with a number of fields.\n",
    "\n",
    "In CSV, each row stores a single data item and, within the row, the field values are listed separated by commas (hence the name CSV). Optionally, the first row of the file can contain the names of the fields.\n",
    "\n",
    "For example, a CSV file of wind farm data might be storing a wind farm ID, the number of turbines, the turbine height, and the maximum kW power of each wind farm. This would be stored in a file called, for example, [data/020/windfarm.csv](data/020/windfarm.csv), with contents like this,\n",
    "\n",
    "```csv\n",
    "\"id\", \"turbines\", \"height\", \"power\"\n",
    "\"WF1355\", 13, 53, 19500\n",
    "\"WF1364\", 3, 60, 8250\n",
    "\"WF1356\", 12, 60, 24000\n",
    "\"WF1357\", 36, 60, 72000\n",
    "```\n",
    "\n",
    "The above example stores data for just four wind farms. In practice, a CSV file might contain thousands or hundreds of thousands of data items. Also, in a real dataset there may be a far greater number of fields, e.g., latitude, longitude, year of installation, turbine manufacturer, etc.\n",
    "\n",
    "### 2.2 Reading a CSV file with `csv.reader`\n",
    "\n",
    "For handling csv files in Python, we can use the `csv` module, which is part of the Standard Library. This has a method called `reader` which will take a file handle and return an **iterator object** that allows us to read each row of the file as a list of field values. In the example below, this has been used in a list comprehension to load the entire dataset as a list of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T15:21:27.495454Z",
     "iopub.status.busy": "2025-10-05T15:21:27.495115Z",
     "iopub.status.idle": "2025-10-05T15:21:27.499127Z",
     "shell.execute_reply": "2025-10-05T15:21:27.498872Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('data/020/windfarm.csv') as csvfile:\n",
    "    windfarm_reader = csv.reader(csvfile, skipinitialspace=True)\n",
    "    # Read each row (as a list) and store them as a list of lists.\n",
    "    data = [row for row in windfarm_reader]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "ðŸ’¡ Tip: In practice, it is good to be explicit about the file encoding, especially when working with real-world data that may include accented or non-ASCII characters. You can do this by passing the argument `encoding='utf-8'` to `open()`, e.g.\n",
    "`with open('data/020/windfarm.csv', encoding='utf-8') as csvfile:`\n",
    "Remember that the file path is interpreted relative to your current working directory, which may differ when running code from a notebook versus a terminal.\n",
    "\n",
    "ðŸ”º Note the use of the parameter setting `skipinitialspace=True`. This tells the reader to ignore **whitespace** (i.e., space and tab characters) that might occur after the comma separators in the file. This is important because if this is not done then spaces in the file would be incorporated into the field values (which is not normally what you want.)\n",
    "\n",
    "We can now access the data, for example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T15:21:27.500496Z",
     "iopub.status.busy": "2025-10-05T15:21:27.500414Z",
     "iopub.status.idle": "2025-10-05T15:21:27.502151Z",
     "shell.execute_reply": "2025-10-05T15:21:27.501942Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print the entire dataset\n",
    "print(data)\n",
    "# Print the 2nd entry\n",
    "print(data[1])\n",
    "# Print the 3rd field of the 2nd entry\n",
    "print(data[1][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "This code has read the data, but the resulting data are not stored in a very convenient format. The data are returned as a list of lists. The first element of the list contains the names of the fields that are stored as a list of strings. Subsequent elements in the list contain the rows of the csv file with each of these rows stored as a simple list of field values. To interpret these lists, we have to remember what the order of the list means (or use the names in `data[0]`). A further issue is that all the field values are being treated as strings, e.g. the power for the first windfarm is 19500 kW, where 19500 is a numeric value but is stored as the character string '19500'. We would need to take extra steps to convert the fields into the correct types.\n",
    "\n",
    "### 2.3 A better way - using `csv.DictReader`\n",
    "\n",
    "Storing our dataset as a list of lists makes it hard to work with. A much more useful representation would be as a **list of dictionaries**. In this case,  each dictionary would store a single data item, and the dictionary entries would store the field values, indexed by their field names. For example, the data would look like this,\n",
    "\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"id\": \"WF1355\",\n",
    "        \"turbines\": 13,\n",
    "        \"height\": 53,\n",
    "        \"power\": 19500\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"WF1364\",\n",
    "        \"turbines\": 3,\n",
    "        \"height\": 60,\n",
    "        \"power\": 8250\n",
    "    },\n",
    "    // etc\n",
    "]\n",
    "```\n",
    "\n",
    "Although this representation has a lot of redundancy (i.e., the field names are repeated for each data item), it is much easier to work with, and the redundancy is not a problem for small datasets. (Later, we will be looking at a package called Pandas that provides much more efficient ways of dealing with very large datasets.)\n",
    "\n",
    "Fortunately, the csv module allows us to read the file into this format very easily. We simply need to replace the `reader` object with a `dictReader`. This is done as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T15:21:27.503269Z",
     "iopub.status.busy": "2025-10-05T15:21:27.503182Z",
     "iopub.status.idle": "2025-10-05T15:21:27.504995Z",
     "shell.execute_reply": "2025-10-05T15:21:27.504789Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('data/020/windfarm.csv') as csvfile:\n",
    "    windfarm_reader = csv.DictReader(csvfile, skipinitialspace=True)\n",
    "    # ... windfarm_reader is now a DictReader which will read each row as a dictionary\n",
    "    # The line below remains exactly the same but now generates a list of dicts.\n",
    "    data = [row for row in windfarm_reader]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "Note how the above code snippet compares with the example in the previous section. We have simply had to replace `csv.reader` with `csv.DictReader`.\n",
    "\n",
    "We can now access the data using list indexing to retrieve data items and dictionary indexing to access specific fields. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T15:21:27.505939Z",
     "iopub.status.busy": "2025-10-05T15:21:27.505880Z",
     "iopub.status.idle": "2025-10-05T15:21:27.507338Z",
     "shell.execute_reply": "2025-10-05T15:21:27.507146Z"
    }
   },
   "outputs": [],
   "source": [
    "# print the 1st entry\n",
    "print(data[0])\n",
    "# print the power output of the first entry\n",
    "print(data[0]['power'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "This is both easier to write (as we do not have to remember the order of the fields in the original file) and much easier to read, i.e., `data[0]['power']` is much more meaningful than `data[0][3]`.\n",
    "\n",
    "### 2.4 Dealing with numeric versus string fields\n",
    "\n",
    "Note, we still have the problem in the example above that all fields are read in and stored as strings.\n",
    "\n",
    "A solution to this is to use the `quoting` parameter of the `DictReader` to specify that unquoted values should be converted into floats; quoted values remain as strings. This is done as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T15:21:27.508344Z",
     "iopub.status.busy": "2025-10-05T15:21:27.508269Z",
     "iopub.status.idle": "2025-10-05T15:21:27.510028Z",
     "shell.execute_reply": "2025-10-05T15:21:27.509831Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('data/020/windfarm.csv') as csvfile:\n",
    "    windfarm_reader = csv.DictReader(csvfile, skipinitialspace=True,  quoting=csv.QUOTE_NONNUMERIC)\n",
    "    # ... windfarm_reader is now a DictReader which will read each row as a dictionary\n",
    "    # The line below remains exactly the same but now generates a list of dicts.\n",
    "    data = [row for row in windfarm_reader]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "So we can now see that the numeric values are being stored as numbers and not strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T15:21:27.510946Z",
     "iopub.status.busy": "2025-10-05T15:21:27.510884Z",
     "iopub.status.idle": "2025-10-05T15:21:27.512321Z",
     "shell.execute_reply": "2025-10-05T15:21:27.512138Z"
    }
   },
   "outputs": [],
   "source": [
    "# print the 1st entry\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "This is still not perfect because all numeric values have been stored as floats. We might have hoped that the csv library could infer whether values are integers or floats by the presence of a decimal point. Unfortunately, this is not the case.\n",
    "\n",
    "We can, however, easily process the data after having read it to convert the values of specific fields to integers if we want to. For example, the following compact piece of code will cast the 'turbine' field (which stores the integer number of turbines in the farm) into an integer value. Note that it is effectively regenerating the list by processing each dictionary in turn. (Do not worry too much about the syntax of this code at this stage.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T15:21:27.513341Z",
     "iopub.status.busy": "2025-10-05T15:21:27.513278Z",
     "iopub.status.idle": "2025-10-05T15:21:27.514822Z",
     "shell.execute_reply": "2025-10-05T15:21:27.514613Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert the power values to integers\n",
    "data = [{**row, 'turbines': int(row['turbines'])} for row in data]\n",
    "\n",
    "# Note how the turbines field is now an integer\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "A more difficult issue is that we have relied on the csv file having quotes around all the string values. In fact, many csv files do not follow this convention, i.e. string-valued fields may appear unquoted. In this case the \"QUOTE_NONNUMERIC\" option would cause the parsing to fail as it would try to convert these string into numeric values.\n",
    "\n",
    "If our csv file has unquoted strings, then the easiest solution is to go back to treating everything as a string value and then doing explicit type conversions ourselves using code like the example above.\n",
    "\n",
    "Fortunately, in the weeks to come, when we look at the Pandas package, we will see more powerful csv readers that will handle a lot of these issues for us.\n",
    "\n",
    "### 2.5 Writing CSV files\n",
    "\n",
    "Just as we can read CSV data using `csv.DictReader`, we can also write data using `csv.DictWriter`. The DictWriter object takes a list of field names that define the column order and can write either individual rows or a complete list of dictionaries. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T15:21:27.515796Z",
     "iopub.status.busy": "2025-10-05T15:21:27.515734Z",
     "iopub.status.idle": "2025-10-05T15:21:27.517520Z",
     "shell.execute_reply": "2025-10-05T15:21:27.517334Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('data/020/windfarm_copy.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=data[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc11e953",
   "metadata": {},
   "source": [
    "The `newline=''` argument prevents extra blank lines from appearing in the output on Windows systems. The resulting file can be opened directly in spreadsheet programs such as Excel or LibreOffice Calc.\n",
    "\n",
    "## 3 The JSON data format\n",
    "\n",
    "The CSV files that we looked at in the last section are convenient when each field in our data entry can be represented by a simple type such as a string or a number. However, in many cases, the field values might themselves have more complex types such as lists or even dictionaries.\n",
    "\n",
    "For example, imagine we have weather data for a large number of cities around the world for a given year. For each city (one data entry), we may have simple fields that store, for example, the name and the country, but the weather data might be recorded for each month, i.e., twelve values. Further, for each month we might have separate recordings for, say, the daily rainfall, minimum and maximum temperature, the number of snowy days, etc, which would be best stored as a dictionary. So, within the data entry, we have a list of twelve months, and each month is in turn stored as a dictionary. Our resulting data might look something like this,\n",
    "\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"city\": \"Amsterdam\",\n",
    "    \"country\": \"Netherlands\",\n",
    "    \"monthlyAvg\": [\n",
    "      {\n",
    "        \"high\": 7,\n",
    "        \"low\": 3,\n",
    "        \"snowDays\": 4,\n",
    "        \"rainfall\": 68\n",
    "      },\n",
    "      {\n",
    "        \"high\": 6,\n",
    "        \"low\": 3,\n",
    "        \"snowDays\": 2,\n",
    "        \"rainfall\": 47\n",
    "      }\n",
    "      // etc\n",
    "    ]\n",
    "  }\n",
    "  // etc\n",
    "]\n",
    "```\n",
    "\n",
    "In the data above, we still have a list of dictionaries, where each dictionary represents a single data entry (that is, one city). However, within a data entry we now have a field \"monthlyAvg\" which is itself a  list of dictionaries (i.e. weather data for each of the twelve months).\n",
    "\n",
    "This kind of nested data structure is not easily represented in a CSV file. For this type of data, we need a more flexible format. One such format is **JSON** (**JavaScript Object Notation**). This is a widely used format for storing data and is used extensively in web applications. It is also a very convenient format for storing data in Python and is now widely used in data science.\n",
    "\n",
    "JSON files are text files which are usually given the file extension '.json'. Their contents look like the text in the example above (in fact, the text above is a snippet of a larger JSON file with '//etc' added where text has been deleted). Dictionary entries are introduced with '{' and terminated with '}'. Lists are introduced with '[' and terminated with ']'. The values in the dictionary entries can be strings, numbers, lists or even other dictionaries. The values in the lists can be strings, numbers, lists or dictionaries. So, JSON files can be used to store arbitrarily complex data structures. Unlike CSV files, which can vary in their format (e.g., do strings have quotes or not? What character is used as the quote? etc.), the JSON format is strictly defined and so although it can store more complex data, there is no ambiguity in how the data are stored. This makes it easy to write tools for reading and writing JSON files, and there are many such tools available.\n",
    "\n",
    "(For the full specification of the JSON format see <https://www.json.org/json-en.html>. Note that in the specification, the term \"object\" is used to refer to what we have been calling a dictionary, and the term \"array\" is used to refer to what we have been calling a list. This is largely due to JSON's origins in JavaScript.)\n",
    "\n",
    "### 3.1 Reading JSON files\n",
    "\n",
    "As with CSV files, Python has a Standard Library module for reading and writing JSON files. The module is called `json` and is imported with `import json`.\n",
    "\n",
    "The `json` module has two main functions, `load` and `dump`. The `load` function is used to read a JSON file and convert it into a Python data structure. The `dump` function is used to convert a Python data structure into a JSON file.\n",
    "\n",
    "In the example below, we use the `load` function to read the JSON file `data/020/climate.json` and convert it into a Python list, which we have called `climate_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34d1d0c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T15:21:27.518458Z",
     "iopub.status.busy": "2025-10-05T15:21:27.518383Z",
     "iopub.status.idle": "2025-10-05T15:21:27.520759Z",
     "shell.execute_reply": "2025-10-05T15:21:27.520584Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('data/020/climate.json') as jsonfile:\n",
    "    climate_data = json.load(jsonfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Notice how this code is even simpler than the CSV reading code. This is largely because the strictness of the JSON format means that the load function does not require any additional configuration.\n",
    "\n",
    "We can now print out the first entry using the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T15:21:27.521703Z",
     "iopub.status.busy": "2025-10-05T15:21:27.521624Z",
     "iopub.status.idle": "2025-10-05T15:21:27.523059Z",
     "shell.execute_reply": "2025-10-05T15:21:27.522874Z"
    }
   },
   "outputs": [],
   "source": [
    "print(climate_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "There are a few things to note when comparing this to the earlier CSV example.\n",
    "\n",
    "- There is now no ambiguity about string vs numeric types because the JSON format strictly insists that all strings are quoted. In fact, it even insists that the double quote (\\\") symbol is used for quoting.\n",
    "- Numeric values can have an integer or float type. In the example above, all values are integers, but if the values had contained a decimal point, then they would have been read as floats.\n",
    "- The JSON reader converts JSON arrays into Python lists. There is no JSON equivalent of a tuple. i.e., in Python tuples are delimited by '(' and ')', but in JSON there is no such notation.\n",
    "- The JSON standard requires that the object keys are strings. This is stricter than a general Python dictionary. Python is happy to have dictionaries that have keys of non-string types (e.g., ints). This means that not every Python dictionary can be converted into a JSON object. i.e., the JSON format is very flexible, but not as flexible as Python. In practice, this is rarely an issue for the types of data that are used in data science.\n",
    "\n",
    "### 3.2 Writing JSON files\n",
    "\n",
    "Writing JSON files is as easy as reading them. To write a JSON-compatible Python data structure to a file, we use the `dump` function. For example, to write out the climate data that we have just read in, we can use,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T15:21:27.523959Z",
     "iopub.status.busy": "2025-10-05T15:21:27.523898Z",
     "iopub.status.idle": "2025-10-05T15:21:27.529769Z",
     "shell.execute_reply": "2025-10-05T15:21:27.529582Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('data/020/climate_copy.json', 'w') as jsonfile:\n",
    "    json.dump(climate_data, jsonfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "We can check the contents of this file,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T15:21:27.530734Z",
     "iopub.status.busy": "2025-10-05T15:21:27.530676Z",
     "iopub.status.idle": "2025-10-05T15:21:27.724437Z",
     "shell.execute_reply": "2025-10-05T15:21:27.724196Z"
    }
   },
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# head data/020/climate_copy.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "(Note, remove the `#` if you are running this in a terminal window. Lines have been commented out so that the long output does not appear in these notes.)\n",
    "\n",
    "Notice that the whole file has appeared on one line. This is nice and compact, but not very human-readable! To make the dump function split the output over lines and indent the nested structure we simply need to pass one additional parameter, `indent=4` (where the value of the parameter specifies how many columns to use when indenting; 4 and 2 are commonly used values). For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T15:21:27.813030Z",
     "iopub.status.busy": "2025-10-05T15:21:27.812878Z",
     "iopub.status.idle": "2025-10-05T15:21:27.820261Z",
     "shell.execute_reply": "2025-10-05T15:21:27.820040Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('data/020/climate_copy.json', 'w') as jsonfile:\n",
    "    json.dump(climate_data, jsonfile, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "We can check the contents of this file by printing the first few lines in a terminal window, e.g., using linux,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T15:21:27.821454Z",
     "iopub.status.busy": "2025-10-05T15:21:27.821378Z",
     "iopub.status.idle": "2025-10-05T15:21:27.846064Z",
     "shell.execute_reply": "2025-10-05T15:21:27.845774Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "head data/020/climate_copy.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "As we mentioned, not all Python data structures can be directly translated into valid JSON. For example, suppose that we had a list of dictionaries that had tuple entries. What would happen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T15:21:27.847332Z",
     "iopub.status.busy": "2025-10-05T15:21:27.847251Z",
     "iopub.status.idle": "2025-10-05T15:21:27.849881Z",
     "shell.execute_reply": "2025-10-05T15:21:27.849690Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "my_data = [{'name': 'Alice', 'favourite_numbers': (1, 2, 3)},\n",
    "           {'name': 'Bob', 'favourite_numbers': (4, 5, 6)}]\n",
    "\n",
    "with open('tmp/numbers.json', 'w') as jsonfile:\n",
    "    json.dump(my_data, jsonfile)\n",
    "\n",
    "with open('tmp/numbers.json') as jsonfile:\n",
    "    my_data_read_back_in = json.load(jsonfile)\n",
    "\n",
    "print('favourite_numbers starts as a:')\n",
    "print(type(my_data[0]['favourite_numbers']))\n",
    "print('favourite_numbers is read back as:')\n",
    "print(type(my_data_read_back_in[0]['favourite_numbers']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "Note that the tuple has been stored as a JSON array, so on reading it is interpreted as a Python list, that is, the `my_data_read_back_in` variable is not the same as the original `my_data` variable. The values stored in both are the same, but different data types are used (i.e. a list versus a tuple). This can lead to subtle bugs if you are not careful.\n",
    "\n",
    "In the more extreme example below, we will try writing a dictionary that uses an integer key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T15:21:27.850844Z",
     "iopub.status.busy": "2025-10-05T15:21:27.850781Z",
     "iopub.status.idle": "2025-10-05T15:21:27.852825Z",
     "shell.execute_reply": "2025-10-05T15:21:27.852624Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "my_data = [{1: 'chocolate', 2: 'vanilla'},\n",
    "            {1: 'raspberry', 2: 'chocolate'}]\n",
    "\n",
    "with open('tmp/icecream.json', 'w') as jsonfile:\n",
    "    json.dump(my_data, jsonfile)\n",
    "\n",
    "with open('tmp/icecream.json') as jsonfile:\n",
    "    my_data_read_back_in = json.load(jsonfile)\n",
    "\n",
    "\n",
    "print(my_data_read_back_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "Note that in this case, the key values were converted to strings. Again, this might lead to problems if we later expected the key values to be numeric.\n",
    "\n",
    "These issues do not usually cause real difficulties, but it is worth being aware of them.\n",
    "\n",
    "### 3.3 JSON Lines format\n",
    "\n",
    "One of the nice features of the CSV format is that each data entry is stored on a separate line. This makes it very easy to read items from large files one at a time, i.e., note how the csv.reader and csv.DictReader objects iterate over a file, line by line, returning a data entry each time.\n",
    "\n",
    "With the standard JSON format this is not possible. JSON does not dictate how white space is used and as we have seen, it is typical to store a single data-entry over many lines using indentation to indicate the nested structure. Even if we insisted on storing each data-entry on a single line there is a further problem: the JSON format for a list of dictionaries requires the whole list to be enclosed in square brackets. This means that a JSON parser will be looking for the closing square bracket before it can return any data. This is not a problem for small files, but for large files it means that the whole file must be read into memory before any data can be returned.\n",
    "\n",
    "To make JSON more friendly for storing large data science datasets, the `JSON Lines` format was developed (typically stored with the file extension `.jsonl`). In this format, we simply store each data entry as a valid JSON string **on a separate line** terminated by a newline character. Note that although each line is a valid JSON object (and can be parsed with json.loads), the file as a whole is not. i.e., the list of data entries is not enclosed in square brackets, and the data entries are not separated by commas. However, this file, like a CSV file, can be read and parsed line by line. This is particularly useful for processing large data science datasets that may be too large to fit into memory.\n",
    "\n",
    "To make this clear, compare the JSON and JSON Lines formats for the ice cream flavour data that we had above.\n",
    "\n",
    "- First, as it might appear in a JSON file...\n",
    "\n",
    "```json\n",
    "[\n",
    " {\n",
    "    \"name\": \"Alice\",\n",
    "    \"favourite_numbers\": [1, 2, 3]\n",
    " },\n",
    " {\n",
    "    \"name\": \"Bob\",\n",
    "    \"favourite_numbers\": [4, 5, 6]\n",
    " }\n",
    "]\n",
    "```\n",
    "\n",
    "- ... and now as it would appear in a JSON Lines file.\n",
    "\n",
    "```json\n",
    " {\"name\": \"Alice\", \"favourite_numbers\": [1, 2, 3]}\n",
    " {\"name\": \"Bob\", \"favourite_numbers\": [4, 5, 6]}\n",
    "```\n",
    "\n",
    "Note that we no longer enclose the items in a list and we no longer separate them with commas. However, we insist that each item has its own line.\n",
    "\n",
    "The JSON Lines format is a very simple convention and the full specification can be found on this single page website, <https://jsonlines.org/>\n",
    "\n",
    "#### 3.3.1 Reading JSON lines files\n",
    "\n",
    "Note, because `.jsonl` is not a valid `.json` file, it cannot be read directly with the builtin `json.load` function. Also, there is no jsonl file reader built into the standard library. However, we can easily read `.jsonl` files with a few lines of Python. To do this, we can use the `json.loads` function, which will convert a single string into a JSON object. Using this, we can read the file line-by-line and convert each line as we go. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T15:21:27.853784Z",
     "iopub.status.busy": "2025-10-05T15:21:27.853720Z",
     "iopub.status.idle": "2025-10-05T15:21:27.855409Z",
     "shell.execute_reply": "2025-10-05T15:21:27.855184Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('data/020/climate.jsonl') as jsonfile:\n",
    "    climate_data = [json.loads(line) for line in jsonfile]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "Note, the example above is not particularly useful as we have still read the entire data into memory. If the dataset were very large we would more typically be processing the data while we read, i.e., we might do something like the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T15:21:27.856342Z",
     "iopub.status.busy": "2025-10-05T15:21:27.856271Z",
     "iopub.status.idle": "2025-10-05T15:21:27.857875Z",
     "shell.execute_reply": "2025-10-05T15:21:27.857676Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('data/020/climate.jsonl') as jsonfile:\n",
    "    for line in jsonfile:\n",
    "        climate_data_entry = json.loads(line)\n",
    "        # do something with this data entry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "#### 3.3.2 Writing JSON lines files\n",
    "\n",
    "Writing files in JSON Lines format is just as easy. We simply need to convert each data entry into a JSON string and write it to a file. The `json.dumps` function will convert the JSON object into a string for us. So, for example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T15:21:27.858777Z",
     "iopub.status.busy": "2025-10-05T15:21:27.858702Z",
     "iopub.status.idle": "2025-10-05T15:21:27.860295Z",
     "shell.execute_reply": "2025-10-05T15:21:27.860093Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('data/020/climate.jsonl', 'w') as jsonfile:\n",
    "    for data_entry in climate_data:\n",
    "        jsonfile.write(json.dumps(data_entry) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "Note how we are adding a newline character ('\\n') to the end of each string. This is important as it will ensure that each data entry is stored on a separate line.\n",
    "\n",
    "In many real-world applications, JSON Lines files are not written all at once but are appended to over time. For example, a data-collection script might record one measurement or log entry at a time and add it as a new line to an existing .jsonl file.\n",
    "\n",
    "To support this, you can open the file in append mode rather than write mode by using 'a' instead of 'w' in the `open()` call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94c27182",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T15:21:27.861276Z",
     "iopub.status.busy": "2025-10-05T15:21:27.861216Z",
     "iopub.status.idle": "2025-10-05T15:21:27.862741Z",
     "shell.execute_reply": "2025-10-05T15:21:27.862553Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('data/020/climate.jsonl', 'a', encoding='utf-8') as jsonfile:\n",
    "    for data_entry in climate_data:\n",
    "        jsonfile.write(json.dumps(data_entry) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ab5f59",
   "metadata": {},
   "source": [
    "Each new line you write is added to the end of the file without re-reading or rewriting earlier data. This line-by-line append behaviour is one of the main reasons JSON Lines is so popular for streaming, incremental, or continuously growing datasets, such as logs, sensor readings, or large-scale web data collections.\n",
    "\n",
    "Note that because the JSON lines format squashes the entire object onto a single line, it is no longer quite as human-readable as the standard JSON format. However, for most popular coding editors (such as VSCode) there are now editor plugins that will allow you to browse JSON lines files in a readable way. The JSON lines format is therefore a great compromise between machine and human readability: It has the flexibility of JSON while allowing the line-by-line processing of CSV.\n",
    "\n",
    "*Copyright Â© 2023â€“2025 Jon Barker, University of Sheffield. All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
